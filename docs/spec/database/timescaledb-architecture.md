# ğŸ“Š TimescaleDB ç²¾ç°¡ç›£æ§æ¶æ§‹è¨­è¨ˆ

## ğŸ¯ æ¶æ§‹æ¦‚è¿°

æœ¬æ–‡æª”è©³ç´°èªªæ˜åŸºæ–¼ TimescaleDB + MySQL çš„ç²¾ç°¡ç›£æ§ç³»çµ±æ¶æ§‹ï¼Œå°ˆç‚ºæ—¥èªŒç›£æ§å’Œ Elasticsearch æœå‹™ç›£æ§è¨­è¨ˆï¼Œå¯¦ç¾ 3 å€‹æœˆæ•¸æ“šä¿ç•™çš„é«˜æ€§èƒ½è¼•é‡ç´šè§£æ±ºæ–¹æ¡ˆã€‚

## ğŸ—ï¸ ç²¾ç°¡é›™å±¤æ¶æ§‹è¨­è¨ˆ

### **ä¸»è¦æ¶æ§‹åœ–**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ æ—¥èªŒç›£æ§     â”‚ ES ç›£æ§æ”¶é›†  â”‚  å‘Šè­¦å¼•æ“   â”‚   Web Dashboard â”‚   â”‚
â”‚  â”‚ Collector   â”‚ ES Collectorâ”‚Alert Engine â”‚      UI         â”‚   â”‚
â”‚  â”‚             â”‚             â”‚             â”‚                 â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚ â”‚BatchWritâ”‚ â”‚ â”‚BatchWritâ”‚ â”‚ â”‚QueryOpt â”‚ â”‚ â”‚InMemory     â”‚ â”‚   â”‚
â”‚  â”‚ â”‚er       â”‚ â”‚ â”‚er       â”‚ â”‚ â”‚imizer   â”‚ â”‚ â”‚Cache        â”‚ â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç²¾ç°¡é›™å±¤æ•¸æ“šå­˜å„²                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚       TimescaleDB           â”‚  â”‚        MySQL            â”‚   â”‚
â”‚  â”‚     (æ™‚é–“åºåˆ—æ•¸æ“š)           â”‚  â”‚    (é…ç½®+ç”¨æˆ¶)           â”‚   â”‚
â”‚  â”‚                             â”‚  â”‚                         â”‚   â”‚
â”‚  â”‚ â€¢ æ‰€æœ‰ç›£æ§æ­·å² (3å€‹æœˆ)       â”‚  â”‚ â€¢ ç”¨æˆ¶èªè­‰              â”‚   â”‚
â”‚  â”‚ â€¢ å‘Šè­¦æ­·å²è¨˜éŒ„             â”‚  â”‚ â€¢ ç›£æ§é…ç½®              â”‚   â”‚
â”‚  â”‚ â€¢ é«˜æ€§èƒ½æ™‚é–“åºåˆ—æŸ¥è©¢        â”‚  â”‚ â€¢ è¨­å‚™ç®¡ç†              â”‚   â”‚
â”‚  â”‚ â€¢ è‡ªå‹•åˆ†å€/å£“ç¸®/æ¸…ç†        â”‚  â”‚ â€¢ æ¬Šé™æ§åˆ¶              â”‚   â”‚
â”‚  â”‚ â€¢ äºç§’ç´šèšåˆçµ±è¨ˆ           â”‚  â”‚ â€¢ Cron ä»»å‹™             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¯é¸æ“´å±• (é«˜ä¸¦ç™¼æ™‚å†åŠ å…¥):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Redis (å¯é¸ç†±æ•¸æ“šå±¤)                          â”‚
â”‚               â€¢ æ¯«ç§’ç´šæŸ¥è©¢ â€¢ é«˜ä½µç™¼æ”¯æ´ â€¢ åˆ†æ•£å¼ç·©å­˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š TimescaleDB è¨­è¨ˆè©³è§£

### **ç‚ºä»€éº¼é¸æ“‡ TimescaleDB**

#### **1. æ€§èƒ½å„ªå‹¢**
```yaml
å¯«å…¥æ€§èƒ½: 500,000+ inserts/sec
æŸ¥è©¢æ€§èƒ½: äºç§’ç´šæ™‚é–“åºåˆ—æŸ¥è©¢
å£“ç¸®ç‡: 90% æ•¸æ“šå£“ç¸® (7å¤©å¾Œè‡ªå‹•å£“ç¸®)
åˆ†å€: è‡ªå‹•æŒ‰æ™‚é–“åˆ†å€ï¼ŒæŸ¥è©¢æ•ˆç‡æ¥µé«˜
```

#### **2. SQL å…¼å®¹**
```sql
-- å®Œå…¨å…¼å®¹ PostgreSQL SQL èªæ³•
SELECT * FROM device_metrics WHERE device_id = 'server1';

-- æ™‚é–“åºåˆ—å°ˆç”¨å‡½æ•¸
SELECT
    time_bucket('5 minutes', time) as interval,
    device_id,
    avg(response_time) as avg_response_time
FROM device_metrics
WHERE time >= NOW() - INTERVAL '24 hours'
GROUP BY interval, device_id
ORDER BY interval DESC;
```

#### **3. è‡ªå‹•åŒ–ç®¡ç†**
```sql
-- è‡ªå‹•æ•¸æ“šä¿ç•™ (3å€‹æœˆå¾Œè‡ªå‹•åˆªé™¤)
SELECT add_retention_policy('device_metrics', INTERVAL '90 days');

-- è‡ªå‹•å£“ç¸® (7å¤©å¾Œå£“ç¸®ï¼Œç¯€çœ90%ç©ºé–“)
SELECT add_compression_policy('device_metrics', INTERVAL '7 days');
```

### **è¡¨çµæ§‹è¨­è¨ˆ**

#### **æ—¥èªŒç›£æ§è¡¨ (device_metrics)**
```sql
CREATE TABLE device_metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id TEXT NOT NULL,
    device_group TEXT NOT NULL,
    logname TEXT NOT NULL,
    status TEXT NOT NULL,
    response_time BIGINT DEFAULT 0,
    lost BOOLEAN DEFAULT FALSE,
    target_id INTEGER,
    index_id INTEGER,
    error_message TEXT,
    metadata JSONB
);

-- è½‰æ›ç‚ºæ™‚é–“åºåˆ—è¡¨
SELECT create_hypertable('device_metrics', 'time', chunk_time_interval => INTERVAL '1 day');

-- è‡ªå‹•ç®¡ç†ç­–ç•¥
ALTER TABLE device_metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'device_id,logname',
    timescaledb.compress_orderby = 'time DESC'
);

SELECT add_compression_policy('device_metrics', INTERVAL '7 days');
SELECT add_retention_policy('device_metrics', INTERVAL '90 days');

-- é«˜æ€§èƒ½ç´¢å¼•
CREATE INDEX idx_device_metrics_device_time ON device_metrics (device_id, time DESC);
CREATE INDEX idx_device_metrics_logname_time ON device_metrics (logname, time DESC);
CREATE INDEX idx_device_metrics_status ON device_metrics (status, time DESC);
CREATE INDEX idx_device_metrics_group ON device_metrics (device_group, time DESC);
```

#### **ES ç›£æ§è¡¨ (es_metrics)**
```sql
CREATE TABLE es_metrics (
    time TIMESTAMPTZ NOT NULL,
    monitor_id INTEGER NOT NULL,
    status TEXT NOT NULL,
    cluster_name TEXT,
    cluster_status TEXT,
    response_time BIGINT DEFAULT 0,
    cpu_usage DECIMAL(5,2) DEFAULT 0.00,
    memory_usage DECIMAL(5,2) DEFAULT 0.00,
    disk_usage DECIMAL(5,2) DEFAULT 0.00,
    node_count INTEGER DEFAULT 0,
    active_shards INTEGER DEFAULT 0,
    unassigned_shards INTEGER DEFAULT 0,
    total_documents BIGINT DEFAULT 0,
    error_message TEXT,
    metadata JSONB
);

SELECT create_hypertable('es_metrics', 'time', chunk_time_interval => INTERVAL '1 day');

ALTER TABLE es_metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'monitor_id',
    timescaledb.compress_orderby = 'time DESC'
);

SELECT add_compression_policy('es_metrics', INTERVAL '7 days');
SELECT add_retention_policy('es_metrics', INTERVAL '90 days');

CREATE INDEX idx_es_metrics_monitor_time ON es_metrics (monitor_id, time DESC);
CREATE INDEX idx_es_metrics_status ON es_metrics (status, time DESC);
CREATE INDEX idx_es_metrics_cluster ON es_metrics (cluster_status, time DESC);
```

#### **å‘Šè­¦æ­·å²è¡¨ (alert_history)**
```sql
CREATE TABLE alert_history (
    time TIMESTAMPTZ NOT NULL,
    monitor_type TEXT NOT NULL, -- 'device' or 'elasticsearch'
    monitor_id INTEGER NOT NULL,
    device_id TEXT,             -- åªæœ‰è¨­å‚™ç›£æ§æ™‚ä½¿ç”¨
    logname TEXT,               -- åªæœ‰è¨­å‚™ç›£æ§æ™‚ä½¿ç”¨
    alert_type TEXT NOT NULL,
    severity TEXT NOT NULL,     -- 'low', 'medium', 'high', 'critical'
    message TEXT NOT NULL,
    status TEXT DEFAULT 'active', -- 'active', 'resolved'
    resolved_at TIMESTAMPTZ,
    resolution_note TEXT
);

SELECT create_hypertable('alert_history', 'time', chunk_time_interval => INTERVAL '7 days');
SELECT add_retention_policy('alert_history', INTERVAL '90 days');

CREATE INDEX idx_alert_history_type_time ON alert_history (monitor_type, time DESC);
CREATE INDEX idx_alert_history_monitor_time ON alert_history (monitor_id, time DESC);
CREATE INDEX idx_alert_history_severity ON alert_history (severity, time DESC);
CREATE INDEX idx_alert_history_status ON alert_history (status, time DESC);
```

## ğŸš€ Go ç¨‹å¼ç¢¼å¯¦ä½œ

### **TimescaleDB é€£æ¥ç®¡ç†**
```go
// database/timescale.go
package database

import (
    "database/sql"
    "fmt"
    "time"

    "gorm.io/driver/postgres"
    "gorm.io/gorm"
    "gorm.io/gorm/logger"
)

type TimescaleDB struct {
    *gorm.DB
    rawDB *sql.DB
}

func NewTimescaleDB(dsn string) (*TimescaleDB, error) {
    // GORM é€£æ¥
    db, err := gorm.Open(postgres.Open(dsn), &gorm.Config{
        Logger: logger.Default.LogMode(logger.Silent), // ç”Ÿç”¢ç’°å¢ƒé—œé–‰æ—¥èªŒ
        PrepareStmt: true,                             // é ç·¨è­¯ SQL
        DisableForeignKeyConstraintWhenMigrating: true,
    })

    if err != nil {
        return nil, fmt.Errorf("failed to connect to TimescaleDB: %v", err)
    }

    // åŸç”Ÿ SQL é€£æ¥ (ç”¨æ–¼æ‰¹é‡æ“ä½œ)
    rawDB, err := db.DB()
    if err != nil {
        return nil, fmt.Errorf("failed to get raw database connection: %v", err)
    }

    // é€£æ¥æ± å„ªåŒ–
    rawDB.SetMaxOpenConns(100)
    rawDB.SetMaxIdleConns(20)
    rawDB.SetConnMaxLifetime(time.Hour)

    return &TimescaleDB{
        DB:    db,
        rawDB: rawDB,
    }, nil
}

// åˆå§‹åŒ–æ™‚é–“åºåˆ—è¡¨
func (t *TimescaleDB) InitializeTables() error {
    queries := []string{
        // å‰µå»ºæ“´å±•
        "CREATE EXTENSION IF NOT EXISTS timescaledb;",

        // è¨­å‚™ç›£æ§è¡¨
        `CREATE TABLE IF NOT EXISTS device_metrics (
            time TIMESTAMPTZ NOT NULL,
            device_id TEXT NOT NULL,
            device_group TEXT NOT NULL,
            logname TEXT NOT NULL,
            status TEXT NOT NULL,
            response_time BIGINT DEFAULT 0,
            lost BOOLEAN DEFAULT FALSE,
            target_id INTEGER,
            index_id INTEGER,
            error_message TEXT,
            metadata JSONB
        );`,

        // ES ç›£æ§è¡¨
        `CREATE TABLE IF NOT EXISTS es_metrics (
            time TIMESTAMPTZ NOT NULL,
            monitor_id INTEGER NOT NULL,
            status TEXT NOT NULL,
            cluster_name TEXT,
            cluster_status TEXT,
            response_time BIGINT DEFAULT 0,
            cpu_usage DECIMAL(5,2) DEFAULT 0.00,
            memory_usage DECIMAL(5,2) DEFAULT 0.00,
            disk_usage DECIMAL(5,2) DEFAULT 0.00,
            node_count INTEGER DEFAULT 0,
            active_shards INTEGER DEFAULT 0,
            unassigned_shards INTEGER DEFAULT 0,
            total_documents BIGINT DEFAULT 0,
            error_message TEXT,
            metadata JSONB
        );`,

        // å‘Šè­¦æ­·å²è¡¨
        `CREATE TABLE IF NOT EXISTS alert_history (
            time TIMESTAMPTZ NOT NULL,
            monitor_type TEXT NOT NULL,
            monitor_id INTEGER NOT NULL,
            device_id TEXT,
            logname TEXT,
            alert_type TEXT NOT NULL,
            severity TEXT NOT NULL,
            message TEXT NOT NULL,
            status TEXT DEFAULT 'active',
            resolved_at TIMESTAMPTZ,
            resolution_note TEXT
        );`,
    }

    for _, query := range queries {
        if err := t.rawDB.QueryRow(query).Err(); err != nil && err != sql.ErrNoRows {
            return fmt.Errorf("failed to execute query %s: %v", query, err)
        }
    }

    // å‰µå»ºæ™‚é–“åºåˆ—è¡¨
    hypertables := []string{
        "SELECT create_hypertable('device_metrics', 'time', chunk_time_interval => INTERVAL '1 day', if_not_exists => TRUE);",
        "SELECT create_hypertable('es_metrics', 'time', chunk_time_interval => INTERVAL '1 day', if_not_exists => TRUE);",
        "SELECT create_hypertable('alert_history', 'time', chunk_time_interval => INTERVAL '7 days', if_not_exists => TRUE);",
    }

    for _, query := range hypertables {
        if _, err := t.rawDB.Exec(query); err != nil {
            return fmt.Errorf("failed to create hypertable: %v", err)
        }
    }

    return t.setupPolicies()
}

// è¨­ç½®è‡ªå‹•ç®¡ç†ç­–ç•¥
func (t *TimescaleDB) setupPolicies() error {
    policies := []string{
        // å£“ç¸®ç­–ç•¥
        `ALTER TABLE device_metrics SET (
            timescaledb.compress,
            timescaledb.compress_segmentby = 'device_id,logname',
            timescaledb.compress_orderby = 'time DESC'
        );`,
        `ALTER TABLE es_metrics SET (
            timescaledb.compress,
            timescaledb.compress_segmentby = 'monitor_id',
            timescaledb.compress_orderby = 'time DESC'
        );`,

        // å£“ç¸®ç­–ç•¥ (7å¤©å¾Œå£“ç¸®)
        "SELECT add_compression_policy('device_metrics', INTERVAL '7 days');",
        "SELECT add_compression_policy('es_metrics', INTERVAL '7 days');",

        // ä¿ç•™ç­–ç•¥ (3å€‹æœˆå¾Œåˆªé™¤)
        "SELECT add_retention_policy('device_metrics', INTERVAL '90 days');",
        "SELECT add_retention_policy('es_metrics', INTERVAL '90 days');",
        "SELECT add_retention_policy('alert_history', INTERVAL '90 days');",
    }

    for _, policy := range policies {
        if _, err := t.rawDB.Exec(policy); err != nil {
            // å¿½ç•¥é‡è¤‡è¨­ç½®çš„éŒ¯èª¤
            if !strings.Contains(err.Error(), "already exists") {
                return fmt.Errorf("failed to setup policy: %v", err)
            }
        }
    }

    return nil
}
```

### **æ‰¹é‡å¯«å…¥æœå‹™**
```go
// services/batch_writer.go
package services

import (
    "context"
    "database/sql"
    "fmt"
    "sync"
    "time"
)

type BatchWriter struct {
    timescaleDB    *sql.DB
    deviceBatch    []DeviceMetric
    esBatch        []ESMetric
    alertBatch     []AlertRecord
    batchSize      int
    flushInterval  time.Duration
    mutex          sync.Mutex
    ticker         *time.Ticker
    stopChan       chan struct{}
}

type DeviceMetric struct {
    Time         time.Time `json:"time"`
    DeviceID     string    `json:"device_id"`
    DeviceGroup  string    `json:"device_group"`
    Logname      string    `json:"logname"`
    Status       string    `json:"status"`
    ResponseTime int64     `json:"response_time"`
    Lost         bool      `json:"lost"`
    TargetID     int       `json:"target_id"`
    IndexID      int       `json:"index_id"`
    ErrorMessage string    `json:"error_message"`
    Metadata     string    `json:"metadata"`
}

type ESMetric struct {
    Time             time.Time `json:"time"`
    MonitorID        int       `json:"monitor_id"`
    Status           string    `json:"status"`
    ClusterName      string    `json:"cluster_name"`
    ClusterStatus    string    `json:"cluster_status"`
    ResponseTime     int64     `json:"response_time"`
    CpuUsage         float64   `json:"cpu_usage"`
    MemoryUsage      float64   `json:"memory_usage"`
    DiskUsage        float64   `json:"disk_usage"`
    NodeCount        int       `json:"node_count"`
    ActiveShards     int       `json:"active_shards"`
    UnassignedShards int       `json:"unassigned_shards"`
    TotalDocuments   int64     `json:"total_documents"`
    ErrorMessage     string    `json:"error_message"`
    Metadata         string    `json:"metadata"`
}

type AlertRecord struct {
    Time           time.Time  `json:"time"`
    MonitorType    string     `json:"monitor_type"`
    MonitorID      int        `json:"monitor_id"`
    DeviceID       string     `json:"device_id,omitempty"`
    Logname        string     `json:"logname,omitempty"`
    AlertType      string     `json:"alert_type"`
    Severity       string     `json:"severity"`
    Message        string     `json:"message"`
    Status         string     `json:"status"`
    ResolvedAt     *time.Time `json:"resolved_at,omitempty"`
    ResolutionNote string     `json:"resolution_note,omitempty"`
}

func NewBatchWriter(db *sql.DB, batchSize int, flushInterval time.Duration) *BatchWriter {
    bw := &BatchWriter{
        timescaleDB:   db,
        deviceBatch:   make([]DeviceMetric, 0, batchSize),
        esBatch:       make([]ESMetric, 0, batchSize),
        alertBatch:    make([]AlertRecord, 0, batchSize),
        batchSize:     batchSize,
        flushInterval: flushInterval,
        ticker:        time.NewTicker(flushInterval),
        stopChan:      make(chan struct{}),
    }

    go bw.startFlushRoutine()
    return bw
}

func (bw *BatchWriter) startFlushRoutine() {
    for {
        select {
        case <-bw.ticker.C:
            bw.flushAllBatches()
        case <-bw.stopChan:
            bw.flushAllBatches()
            return
        }
    }
}

// æ·»åŠ è¨­å‚™ç›£æ§æ•¸æ“š
func (bw *BatchWriter) AddDeviceMetric(metric DeviceMetric) {
    bw.mutex.Lock()
    defer bw.mutex.Unlock()

    bw.deviceBatch = append(bw.deviceBatch, metric)

    if len(bw.deviceBatch) >= bw.batchSize {
        bw.flushDeviceMetrics()
    }
}

// æ·»åŠ  ES ç›£æ§æ•¸æ“š
func (bw *BatchWriter) AddESMetric(metric ESMetric) {
    bw.mutex.Lock()
    defer bw.mutex.Unlock()

    bw.esBatch = append(bw.esBatch, metric)

    if len(bw.esBatch) >= bw.batchSize {
        bw.flushESMetrics()
    }
}

// æ·»åŠ å‘Šè­¦æ•¸æ“š
func (bw *BatchWriter) AddAlert(alert AlertRecord) {
    bw.mutex.Lock()
    defer bw.mutex.Unlock()

    bw.alertBatch = append(bw.alertBatch, alert)

    if len(bw.alertBatch) >= bw.batchSize {
        bw.flushAlerts()
    }
}

func (bw *BatchWriter) flushAllBatches() {
    bw.mutex.Lock()
    defer bw.mutex.Unlock()

    bw.flushDeviceMetrics()
    bw.flushESMetrics()
    bw.flushAlerts()
}

func (bw *BatchWriter) flushDeviceMetrics() {
    if len(bw.deviceBatch) == 0 {
        return
    }

    tx, err := bw.timescaleDB.Begin()
    if err != nil {
        fmt.Printf("Failed to begin transaction for device metrics: %v\n", err)
        return
    }
    defer tx.Rollback()

    stmt, err := tx.Prepare(`
        INSERT INTO device_metrics
        (time, device_id, device_group, logname, status, response_time, lost, target_id, index_id, error_message, metadata)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
    `)
    if err != nil {
        fmt.Printf("Failed to prepare device metrics statement: %v\n", err)
        return
    }
    defer stmt.Close()

    for _, metric := range bw.deviceBatch {
        _, err := stmt.Exec(
            metric.Time, metric.DeviceID, metric.DeviceGroup, metric.Logname,
            metric.Status, metric.ResponseTime, metric.Lost, metric.TargetID,
            metric.IndexID, metric.ErrorMessage, metric.Metadata,
        )
        if err != nil {
            fmt.Printf("Failed to insert device metric: %v\n", err)
            continue
        }
    }

    if err := tx.Commit(); err != nil {
        fmt.Printf("Failed to commit device metrics: %v\n", err)
        return
    }

    fmt.Printf("Successfully flushed %d device metrics\n", len(bw.deviceBatch))
    bw.deviceBatch = bw.deviceBatch[:0] // æ¸…ç©ºæ‰¹æ¬¡
}

func (bw *BatchWriter) flushESMetrics() {
    if len(bw.esBatch) == 0 {
        return
    }

    tx, err := bw.timescaleDB.Begin()
    if err != nil {
        fmt.Printf("Failed to begin transaction for ES metrics: %v\n", err)
        return
    }
    defer tx.Rollback()

    stmt, err := tx.Prepare(`
        INSERT INTO es_metrics
        (time, monitor_id, status, cluster_name, cluster_status, response_time,
         cpu_usage, memory_usage, disk_usage, node_count, active_shards,
         unassigned_shards, total_documents, error_message, metadata)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
    `)
    if err != nil {
        fmt.Printf("Failed to prepare ES metrics statement: %v\n", err)
        return
    }
    defer stmt.Close()

    for _, metric := range bw.esBatch {
        _, err := stmt.Exec(
            metric.Time, metric.MonitorID, metric.Status, metric.ClusterName,
            metric.ClusterStatus, metric.ResponseTime, metric.CpuUsage,
            metric.MemoryUsage, metric.DiskUsage, metric.NodeCount,
            metric.ActiveShards, metric.UnassignedShards, metric.TotalDocuments,
            metric.ErrorMessage, metric.Metadata,
        )
        if err != nil {
            fmt.Printf("Failed to insert ES metric: %v\n", err)
            continue
        }
    }

    if err := tx.Commit(); err != nil {
        fmt.Printf("Failed to commit ES metrics: %v\n", err)
        return
    }

    fmt.Printf("Successfully flushed %d ES metrics\n", len(bw.esBatch))
    bw.esBatch = bw.esBatch[:0] // æ¸…ç©ºæ‰¹æ¬¡
}

func (bw *BatchWriter) flushAlerts() {
    if len(bw.alertBatch) == 0 {
        return
    }

    tx, err := bw.timescaleDB.Begin()
    if err != nil {
        fmt.Printf("Failed to begin transaction for alerts: %v\n", err)
        return
    }
    defer tx.Rollback()

    stmt, err := tx.Prepare(`
        INSERT INTO alert_history
        (time, monitor_type, monitor_id, device_id, logname, alert_type,
         severity, message, status, resolved_at, resolution_note)
        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
    `)
    if err != nil {
        fmt.Printf("Failed to prepare alert statement: %v\n", err)
        return
    }
    defer stmt.Close()

    for _, alert := range bw.alertBatch {
        _, err := stmt.Exec(
            alert.Time, alert.MonitorType, alert.MonitorID, alert.DeviceID,
            alert.Logname, alert.AlertType, alert.Severity, alert.Message,
            alert.Status, alert.ResolvedAt, alert.ResolutionNote,
        )
        if err != nil {
            fmt.Printf("Failed to insert alert: %v\n", err)
            continue
        }
    }

    if err := tx.Commit(); err != nil {
        fmt.Printf("Failed to commit alerts: %v\n", err)
        return
    }

    fmt.Printf("Successfully flushed %d alerts\n", len(bw.alertBatch))
    bw.alertBatch = bw.alertBatch[:0] // æ¸…ç©ºæ‰¹æ¬¡
}

func (bw *BatchWriter) Stop() {
    bw.ticker.Stop()
    close(bw.stopChan)
}
```

### **å…§å­˜ç·©å­˜ç®¡ç† (å¯é¸ Redis é›†æˆ)**
```go
// services/cache_manager.go (å¯é¸å¯¦ä½œ)
package services

import (
    "context"
    "encoding/json"
    "fmt"
    "sync"
    "time"
)

// ç°¡å–®çš„å…§å­˜ç·©å­˜å¯¦ç¾
type MemoryCache struct {
    data map[string]CacheItem
    mutex sync.RWMutex
}

type CacheItem struct {
    Value      interface{}
    Expiration time.Time
}

func NewMemoryCache() *MemoryCache {
    cache := &MemoryCache{
        data: make(map[string]CacheItem),
    }
    // å®šæœŸæ¸…ç†éæœŸæ•¸æ“š
    go cache.cleanup()
    return cache
}

func (c *MemoryCache) Set(key string, value interface{}, ttl time.Duration) {
    c.mutex.Lock()
    defer c.mutex.Unlock()

    c.data[key] = CacheItem{
        Value:      value,
        Expiration: time.Now().Add(ttl),
    }
}

func (c *MemoryCache) Get(key string) (interface{}, bool) {
    c.mutex.RLock()
    defer c.mutex.RUnlock()

    item, exists := c.data[key]
    if !exists || time.Now().After(item.Expiration) {
        return nil, false
    }
    return item.Value, true
}

func (c *MemoryCache) cleanup() {
    ticker := time.NewTicker(5 * time.Minute)
    defer ticker.Stop()

    for range ticker.C {
        c.mutex.Lock()
        now := time.Now()
        for key, item := range c.data {
            if now.After(item.Expiration) {
                delete(c.data, key)
            }
        }
        c.mutex.Unlock()
    }
}

// é©é…å™¨æ¨¡å¼ï¼šæ”¯æ´ Redis æˆ–å…§å­˜ç·©å­˜
type CacheAdapter interface {
    SetDeviceStatus(logname, deviceID string, data interface{}) error
    GetDeviceStatus(logname, deviceID string) (interface{}, error)
    SetESStatus(monitorID int, data interface{}) error
    GetESStatus(monitorID int) (interface{}, error)
}

// å…§å­˜ç·©å­˜é©é…å™¨
type MemoryCacheAdapter struct {
    cache *MemoryCache
}

func NewMemoryCacheAdapter() *MemoryCacheAdapter {
    return &MemoryCacheAdapter{
        cache: NewMemoryCache(),
    }
}

func (m *MemoryCacheAdapter) SetDeviceStatus(logname, deviceID string, data interface{}) error {
    key := fmt.Sprintf("device:%s:%s", logname, deviceID)
    m.cache.Set(key, data, time.Hour)
    return nil
}

func (m *MemoryCacheAdapter) GetDeviceStatus(logname, deviceID string) (interface{}, error) {
    key := fmt.Sprintf("device:%s:%s", logname, deviceID)
    if value, exists := m.cache.Get(key); exists {
        return value, nil
    }
    return nil, fmt.Errorf("not found")
}

func (m *MemoryCacheAdapter) SetESStatus(monitorID int, data interface{}) error {
    key := fmt.Sprintf("es:%d", monitorID)
    m.cache.Set(key, data, time.Hour)
    return nil
}

func (m *MemoryCacheAdapter) GetESStatus(monitorID int) (interface{}, error) {
    key := fmt.Sprintf("es:%d", monitorID)
    if value, exists := m.cache.Get(key); exists {
        return value, nil
    }
    return nil, fmt.Errorf("not found")
}

// Redis é©é…å™¨ (ç•¶éœ€è¦æ™‚æ‰å¯¦ä½œ)
// type RedisCacheAdapter struct {
//     redis *redis.Client
// }
```

### **é›™å±¤æŸ¥è©¢æœå‹™**
```go
// services/query_service.go
package services

import (
    "database/sql"
    "fmt"
    "time"
)

type QueryService struct {
    timescaleDB   *sql.DB
    cacheAdapter  CacheAdapter
}

type TimeRange struct {
    Start time.Time
    End   time.Time
}

func (tr TimeRange) IsRecent() bool {
    return time.Since(tr.Start) <= time.Hour
}

func NewQueryService(timescaleDB *sql.DB, cacheAdapter CacheAdapter) *QueryService {
    return &QueryService{
        timescaleDB:  timescaleDB,
        cacheAdapter: cacheAdapter,
    }
}

// æ™ºèƒ½æŸ¥è©¢ï¼šæ ¹æ“šæ™‚é–“ç¯„åœé¸æ“‡æœ€ä½³æ•¸æ“šæº
func (q *QueryService) GetDeviceHistory(logname, deviceID string, timeRange TimeRange) ([]DeviceMetric, error) {
    // 1. å¦‚æœæŸ¥è©¢æœ€è¿‘æ•¸æ“šï¼Œå„ªå…ˆå¾ç·©å­˜ç²å–
    if timeRange.IsRecent() && q.cacheAdapter != nil {
        if data, err := q.cacheAdapter.GetDeviceStatus(logname, deviceID); err == nil {
            // è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            if metric, ok := data.(DeviceMetric); ok {
                return []DeviceMetric{metric}, nil
            }
        }
    }

    // 2. å¾ TimescaleDB æŸ¥è©¢
    return q.getDeviceHistoryFromTimescale(logname, deviceID, timeRange)
}

func (q *QueryService) getDeviceHistoryFromTimescale(logname, deviceID string, timeRange TimeRange) ([]DeviceMetric, error) {
    query := `
        SELECT time, device_id, device_group, logname, status,
               response_time, lost, target_id, index_id, error_message
        FROM device_metrics
        WHERE logname = $1
            AND device_id = $2
            AND time >= $3
            AND time <= $4
        ORDER BY time DESC
        LIMIT 1000
    `

    rows, err := q.timescaleDB.Query(query, logname, deviceID, timeRange.Start, timeRange.End)
    if err != nil {
        return nil, fmt.Errorf("failed to query device history: %v", err)
    }
    defer rows.Close()

    var results []DeviceMetric
    for rows.Next() {
        var metric DeviceMetric
        var targetID, indexID sql.NullInt64
        var errorMessage sql.NullString

        err := rows.Scan(
            &metric.Time, &metric.DeviceID, &metric.DeviceGroup, &metric.Logname,
            &metric.Status, &metric.ResponseTime, &metric.Lost, &targetID,
            &indexID, &errorMessage,
        )
        if err != nil {
            continue
        }

        if targetID.Valid {
            metric.TargetID = int(targetID.Int64)
        }
        if indexID.Valid {
            metric.IndexID = int(indexID.Int64)
        }
        if errorMessage.Valid {
            metric.ErrorMessage = errorMessage.String
        }

        results = append(results, metric)
    }

    return results, nil
}

// èšåˆæŸ¥è©¢ï¼šç²å–è¨­å‚™çµ±è¨ˆæ•¸æ“š
func (q *QueryService) GetDeviceStats(logname string, timeRange TimeRange) (*DeviceStats, error) {
    // ç›´æ¥å¾ TimescaleDB èšåˆæŸ¥è©¢
    query := `
        SELECT
            COUNT(DISTINCT device_id) as total_devices,
            COUNT(*) as total_checks,
            COUNT(*) FILTER (WHERE status = 'online' AND NOT lost) as online_checks,
            COUNT(*) FILTER (WHERE status = 'offline' OR lost) as offline_checks,
            AVG(response_time) as avg_response_time,
            MAX(response_time) as max_response_time
        FROM device_metrics
        WHERE logname = $1
            AND time >= $2
            AND time <= $3
    `

    row := q.timescaleDB.QueryRow(query, logname, timeRange.Start, timeRange.End)

    var stats DeviceStats
    err := row.Scan(
        &stats.TotalDevices, &stats.TotalChecks, &stats.OnlineChecks,
        &stats.OfflineChecks, &stats.AvgResponseTime, &stats.MaxResponseTime,
    )

    if err != nil {
        return nil, fmt.Errorf("failed to get device stats: %v", err)
    }

    // è¨ˆç®—åœ¨ç·šç‡
    if stats.TotalChecks > 0 {
        stats.UptimeRate = float64(stats.OnlineChecks) / float64(stats.TotalChecks) * 100
    }

    return &stats, nil
}

type DeviceStats struct {
    TotalDevices    int     `json:"total_devices"`
    TotalChecks     int     `json:"total_checks"`
    OnlineChecks    int     `json:"online_checks"`
    OfflineChecks   int     `json:"offline_checks"`
    UptimeRate      float64 `json:"uptime_rate"`
    AvgResponseTime float64 `json:"avg_response_time"`
    MaxResponseTime float64 `json:"max_response_time"`
}

func parseGroupStats(stats map[string]string) *DeviceStats {
    // å¾ Redis hash è§£æçµ±è¨ˆæ•¸æ“š
    // å¯¦ä½œçœç•¥...
    return &DeviceStats{}
}

// æ™‚é–“åºåˆ—èšåˆæŸ¥è©¢
func (q *QueryService) GetDeviceTimeSeries(logname, deviceID string, timeRange TimeRange, interval string) ([]TimeSeriesPoint, error) {
    query := fmt.Sprintf(`
        SELECT
            time_bucket('%s', time) as bucket,
            AVG(response_time) as avg_response_time,
            COUNT(*) as data_points,
            COUNT(*) FILTER (WHERE status = 'online' AND NOT lost) as online_count
        FROM device_metrics
        WHERE logname = $1
            AND device_id = $2
            AND time >= $3
            AND time <= $4
        GROUP BY bucket
        ORDER BY bucket DESC
    `, interval)

    rows, err := q.timescaleDB.Query(query, logname, deviceID, timeRange.Start, timeRange.End)
    if err != nil {
        return nil, fmt.Errorf("failed to query time series: %v", err)
    }
    defer rows.Close()

    var results []TimeSeriesPoint
    for rows.Next() {
        var point TimeSeriesPoint
        err := rows.Scan(&point.Time, &point.AvgResponseTime, &point.DataPoints, &point.OnlineCount)
        if err != nil {
            continue
        }
        results = append(results, point)
    }

    return results, nil
}

type TimeSeriesPoint struct {
    Time            time.Time `json:"time"`
    AvgResponseTime float64   `json:"avg_response_time"`
    DataPoints      int       `json:"data_points"`
    OnlineCount     int       `json:"online_count"`
}
```

## ğŸ“‹ éƒ¨ç½²å’Œé‹ç¶­

### **Docker éƒ¨ç½²é…ç½®**
```yaml
# docker-compose.yml
version: '3.8'
services:
  # æ‡‰ç”¨æœå‹™
  log-detect:
    build: .
    ports:
      - "8006:8006"
    environment:
      - TIMESCALE_URL=postgresql://monitor:password@timescaledb:5432/monitoring
      - MYSQL_URL=mysql://root:password@mysql:3306/config
      - BATCH_SIZE=100
      - BATCH_TIMEOUT=30s
      # Redis å¯é¸é…ç½® (éœ€è¦æ™‚å•Ÿç”¨)
      # - REDIS_URL=redis://redis:6379
    depends_on:
      - timescaledb
      - mysql

  # TimescaleDB (ä¸»è¦æ™‚é–“åºåˆ—æ•¸æ“šå­˜å„²)
  timescaledb:
    image: timescale/timescaledb:latest-pg14
    environment:
      POSTGRES_DB: monitoring
      POSTGRES_USER: monitor
      POSTGRES_PASSWORD: password
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    volumes:
      - timescale_data:/var/lib/postgresql/data
      - ./init-timescale.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    command: >
      postgres
      -c shared_preload_libraries=timescaledb
      -c max_connections=200
      -c work_mem=256MB
      -c maintenance_work_mem=512MB
      -c effective_cache_size=2GB

  # MySQL (é…ç½®å’Œç”¨æˆ¶æ•¸æ“š)
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: config
      MYSQL_COLLATION_SERVER: utf8mb4_unicode_ci
    volumes:
      - mysql_data:/var/lib/mysql
    ports:
      - "3306:3306"

  # Redis (å¯é¸ - é«˜ä¸¦ç™¼æ™‚å†å•Ÿç”¨)
  # redis:
  #   image: redis:7-alpine
  #   volumes:
  #     - redis_data:/data
  #     - ./redis.conf:/usr/local/etc/redis/redis.conf
  #   ports:
  #     - "6379:6379"
  #   command: redis-server /usr/local/etc/redis/redis.conf

volumes:
  timescale_data:
  mysql_data:
  # redis_data: # Redis ç›¸é—œæ™‚å†å•Ÿç”¨
```

### **å¯é¸ Redis é…ç½® (é«˜ä¸¦ç™¼æ™‚å•Ÿç”¨)**
```conf
# redis.conf (å¯é¸é…ç½®æ–‡ä»¶)
# å…§å­˜å„ªåŒ–
maxmemory 2gb
maxmemory-policy allkeys-lru

# æŒä¹…åŒ–é…ç½®
save 900 1
save 300 10
save 60 10000

# ç¶²çµ¡å„ªåŒ–
tcp-keepalive 300
timeout 0

# æ—¥èªŒé…ç½®
loglevel notice
logfile ""
```

### **æ€§èƒ½ç›£æ§è…³æœ¬**
```bash
#!/bin/bash
# monitor.sh - ç³»çµ±æ€§èƒ½ç›£æ§

echo "=== TimescaleDB æ€§èƒ½ç›£æ§ ==="

# 1. æª¢æŸ¥è¡¨å¤§å°å’Œå£“ç¸®ç‡
docker exec timescaledb psql -U monitor -d monitoring -c "
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY size_bytes DESC;
"

# 2. æª¢æŸ¥å£“ç¸®æ•ˆç‡
docker exec timescaledb psql -U monitor -d monitoring -c "
SELECT
    chunk_schema,
    chunk_name,
    pg_size_pretty(before_compression_bytes) as before,
    pg_size_pretty(after_compression_bytes) as after,
    ROUND((before_compression_bytes - after_compression_bytes) * 100.0 / before_compression_bytes, 1) as compression_ratio
FROM timescaledb_information.compression_settings
WHERE before_compression_bytes > 0;
"

# 3. æª¢æŸ¥åˆ†å€æ•¸é‡
docker exec timescaledb psql -U monitor -d monitoring -c "
SELECT
    hypertable_name,
    COUNT(*) as chunk_count,
    pg_size_pretty(SUM(total_bytes)) as total_size
FROM timescaledb_information.chunks
GROUP BY hypertable_name;
"

# Redis æ€§èƒ½ç›£æ§ (å¯é¸ - å•Ÿç”¨ Redis æ™‚æ‰åŸ·è¡Œ)
# echo "=== Redis æ€§èƒ½ç›£æ§ ==="
# docker exec redis redis-cli info memory | grep -E "used_memory_human|maxmemory_human"
# docker exec redis redis-cli info clients
# docker exec redis redis-cli --hotkeys

echo "=== æ‡‰ç”¨æ€§èƒ½ç›£æ§ ==="

# 7. æª¢æŸ¥æ‰¹é‡å¯«å…¥æ€§èƒ½
docker logs log-detect_log-detect_1 | grep "Successfully flushed" | tail -10
```

## ğŸ¯ æ€§èƒ½å„ªåŒ–å»ºè­°

### **TimescaleDB èª¿å„ª**
```sql
-- 1. æŸ¥è©¢æ€§èƒ½å„ªåŒ–
ANALYZE device_metrics;
ANALYZE es_metrics;
ANALYZE alert_history;

-- 2. æª¢æŸ¥æ…¢æŸ¥è©¢
SELECT query, mean_time, calls
FROM pg_stat_statements
WHERE mean_time > 1000
ORDER BY mean_time DESC;

-- 3. ç´¢å¼•ä½¿ç”¨æƒ…æ³
SELECT
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
```

### **æ‡‰ç”¨å±¤å„ªåŒ–**
```go
// 1. TimescaleDB é€£æ¥æ± å„ªåŒ–
db.SetMaxOpenConns(100)          // æœ€å¤§é€£æ¥æ•¸
db.SetMaxIdleConns(20)           // æœ€å¤§ç©ºé–’é€£æ¥
db.SetConnMaxLifetime(time.Hour) // é€£æ¥æœ€å¤§ç”Ÿå‘½é€±æœŸ

// 2. æ‰¹é‡å¯«å…¥å„ªåŒ–
batchSize := 100                 // æ‰¹æ¬¡å¤§å°
flushInterval := 30 * time.Second // åˆ·æ–°é–“éš”

// 3. å…§å­˜ç·©å­˜å„ªåŒ–
cacheCleanupInterval := 5 * time.Minute // æ¸…ç†é–“éš”
cacheTTL := time.Hour                   // ç·©å­˜ç”Ÿå­˜æ™‚é–“

// 4. Redis é€£æ¥æ±  (å¯é¸å•Ÿç”¨)
// redisOptions := &redis.Options{
//     PoolSize:     100,
//     MinIdleConns: 20,
//     MaxRetries:   3,
// }
```

## ğŸ“Š ç¸½çµ

**ç²¾ç°¡é›™å±¤æ¶æ§‹**æä¾›äº†ï¼š

### **æ ¸å¿ƒå„ªå‹¢**
1. **æ¥µé«˜æ€§èƒ½**: TimescaleDB 50è¬+ writes/secï¼Œäºç§’ç´šæŸ¥è©¢
2. **è‡ªå‹•ç®¡ç†**: åˆ†å€ã€å£“ç¸®ã€æ¸…ç†å…¨è‡ªå‹•
3. **SQL å…¼å®¹**: ç„¡å­¸ç¿’æˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨ç¾æœ‰æŠ€èƒ½
4. **æˆæœ¬æ§åˆ¶**: 90% å£“ç¸®ç‡ï¼Œ3å€‹æœˆæ•¸æ“šç”Ÿå‘½é€±æœŸ
5. **éƒ¨ç½²ç°¡åŒ–**: åƒ…éœ€ 2 å€‹æ•¸æ“šåº«æœå‹™ï¼Œç¶­è­·æˆæœ¬ä½

### **æ¶æ§‹ç‰¹é»**
- **TimescaleDB**: è™•ç†é«˜é »æ™‚é–“åºåˆ—æ•¸æ“šï¼Œè‡ªå‹•å„ªåŒ–æ€§èƒ½
- **MySQL**: ç¹¼çºŒè™•ç†é…ç½®å’Œç”¨æˆ¶æ•¸æ“šï¼Œä¿æŒç¾æœ‰é‚è¼¯
- **å…§å­˜ç·©å­˜**: æ‡‰ç”¨å…§ç·©å­˜æä¾›åŸºæœ¬æ€§èƒ½å„ªåŒ–
- **Redis å¯é¸**: é«˜ä¸¦ç™¼éœ€æ±‚æ™‚å†åŠ å…¥ï¼Œç„¡å¼·åˆ¶ä¾è³´

### **æ“´å±•å½ˆæ€§**
- èµ·æ­¥ç°¡å–®ï¼šåƒ…éœ€å…©å±¤æ¶æ§‹å³å¯æ»¿è¶³å¤§éƒ¨åˆ†éœ€æ±‚
- æŒ‰éœ€æ“´å±•ï¼šç•¶ä¸¦ç™¼å¢åŠ æ™‚ï¼Œå¯è¼•é¬†åŠ å…¥ Redis å±¤
- å‘å¾Œå…¼å®¹ï¼šç¾æœ‰ MySQL é‚è¼¯å®Œå…¨ä¿ç•™

é€™å€‹ç²¾ç°¡æ¶æ§‹æ—¢ä¿è­‰äº†ç›£æ§ç³»çµ±çš„é«˜æ€§èƒ½éœ€æ±‚ï¼Œåˆé™ä½äº†éƒ¨ç½²å’Œç¶­è­·çš„è¤‡é›œåº¦ï¼

---

**ç‰ˆæœ¬**: 1.0
**æœ€å¾Œæ›´æ–°**: 2024-09-30
**ä½œè€…**: Log Detect é–‹ç™¼åœ˜éšŠ